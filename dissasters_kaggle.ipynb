{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/wenceslai/Documents/dissasters_kaggle'\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "sid          0.000000\nkeyword      0.796813\nlocation    33.864542\ntext         0.000000\ndtype: float64"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train.isnull().sum() / train.shape[0] * 100\n",
    "test.isnull().sum() / test.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('location', axis=1)\n",
    "test = test.drop('location', axis=1)\n",
    "train = train.drop(train[train['keyword'].isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "id         7552\nkeyword     221\ntext       7447\ntarget        2\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "31"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "max_word_cnt = 0\n",
    "\n",
    "for tweet in pd.concat([test['text'], train['text']]):\n",
    "\n",
    "    words = len(tweet.split())\n",
    "    if words > max_word_cnt:\n",
    "        max_word_cnt = words\n",
    "\n",
    "max_word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nunique tokens:  22653\n"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 10000\n",
    "max_len = max_word_cnt\n",
    "tokenizer = Tokenizer(num_words=max_words, )\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "sequences = tokenizer.texts_to_sequences(train['text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique tokens: \", len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "#shuffling\n",
    "indicies = np.arange(data.shape[0])\n",
    "np.random.shuffle(indicies)\n",
    "data = data[indicies]\n",
    "\n",
    "labels = train['target'].values\n",
    "labels = labels[indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import models, layers, regularizers\n",
    "\n",
    "def get_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(max_words, 20, input_length=max_len))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    #model.add(layers.LSTM(32))\n",
    "    #model.add(layers.LSTM(32))\n",
    "\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-2)))\n",
    "    model.add(layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(1e-2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nTrain on 6041 samples, validate on 1511 samples\nEpoch 1/5\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From /home/wenceslai/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n - 2s - loss: 0.8425 - acc: 0.5982 - val_loss: 0.6828 - val_acc: 0.6353\nEpoch 2/5\n - 1s - loss: 0.6403 - acc: 0.7204 - val_loss: 0.5930 - val_acc: 0.7657\nEpoch 3/5\n - 1s - loss: 0.5574 - acc: 0.7967 - val_loss: 0.5307 - val_acc: 0.7968\nEpoch 4/5\n - 1s - loss: 0.5023 - acc: 0.8219 - val_loss: 0.4990 - val_acc: 0.8154\nEpoch 5/5\n - 1s - loss: 0.4579 - acc: 0.8436 - val_loss: 0.4856 - val_acc: 0.8187\nTrain on 6041 samples, validate on 1511 samples\nEpoch 1/5\n - 2s - loss: 0.8357 - acc: 0.5723 - val_loss: 0.6857 - val_acc: 0.5725\nEpoch 2/5\n - 1s - loss: 0.6517 - acc: 0.6727 - val_loss: 0.6250 - val_acc: 0.6929\nEpoch 3/5\n - 1s - loss: 0.6015 - acc: 0.7777 - val_loss: 0.5812 - val_acc: 0.7743\nEpoch 4/5\n - 1s - loss: 0.5518 - acc: 0.8161 - val_loss: 0.5544 - val_acc: 0.7915\nEpoch 5/5\n - 1s - loss: 0.5233 - acc: 0.8292 - val_loss: 0.5319 - val_acc: 0.8213\nTrain on 6042 samples, validate on 1510 samples\nEpoch 1/5\n - 2s - loss: 0.8454 - acc: 0.6003 - val_loss: 0.6825 - val_acc: 0.6477\nEpoch 2/5\n - 1s - loss: 0.6278 - acc: 0.7251 - val_loss: 0.5670 - val_acc: 0.7927\nEpoch 3/5\n - 1s - loss: 0.5443 - acc: 0.7984 - val_loss: 0.5188 - val_acc: 0.8066\nEpoch 4/5\n - 1s - loss: 0.4849 - acc: 0.8358 - val_loss: 0.4950 - val_acc: 0.8139\nEpoch 5/5\n - 1s - loss: 0.4506 - acc: 0.8482 - val_loss: 0.4896 - val_acc: 0.8185\nTrain on 6042 samples, validate on 1510 samples\nEpoch 1/5\n - 3s - loss: 0.8442 - acc: 0.5841 - val_loss: 0.6706 - val_acc: 0.6722\nEpoch 2/5\n - 1s - loss: 0.6265 - acc: 0.7329 - val_loss: 0.5748 - val_acc: 0.7755\nEpoch 3/5\n - 1s - loss: 0.5354 - acc: 0.8045 - val_loss: 0.5460 - val_acc: 0.7881\nEpoch 4/5\n - 1s - loss: 0.4823 - acc: 0.8297 - val_loss: 0.5304 - val_acc: 0.7801\nEpoch 5/5\n - 1s - loss: 0.4398 - acc: 0.8517 - val_loss: 0.5281 - val_acc: 0.7861\nTrain on 6042 samples, validate on 1510 samples\nEpoch 1/5\n - 2s - loss: 0.8436 - acc: 0.6175 - val_loss: 0.6840 - val_acc: 0.6788\nEpoch 2/5\n - 1s - loss: 0.6181 - acc: 0.7488 - val_loss: 0.6002 - val_acc: 0.7397\nEpoch 3/5\n - 1s - loss: 0.5279 - acc: 0.8140 - val_loss: 0.5637 - val_acc: 0.7695\nEpoch 4/5\n - 1s - loss: 0.4744 - acc: 0.8461 - val_loss: 0.5520 - val_acc: 0.7715\nEpoch 5/5\n - 1s - loss: 0.4353 - acc: 0.8572 - val_loss: 0.5465 - val_acc: 0.7854\ntrain_acc: 0.7577189590464015 val_acc: 0.7564374980939289\n\ntop_val_acc: 0.8064053015550539\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 3\n",
    "epochs = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "skf.get_n_splits(data, labels)\n",
    "\n",
    "val_acc = train_acc = top_val_acc = 0 \n",
    "\n",
    "for train_i, val_i in skf.split(data, labels):\n",
    "    X_train = data[train_i]\n",
    "    X_val = data[val_i]\n",
    "\n",
    "    y_train = labels[train_i]\n",
    "    y_val = labels[val_i]\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        verbose=2\n",
    "        )\n",
    "\n",
    "    val_acc += sum(history.history['val_acc'])\n",
    "    train_acc += sum(history.history['acc']) \n",
    "    top_val_acc += max(history.history['val_acc'])\n",
    "    \n",
    "print(\"train_acc:\", train_acc / (n_splits * epochs), \"val_acc:\", val_acc / (n_splits * epochs))\n",
    "print(\"\\ntop_val_acc:\", top_val_acc / n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train samples: 7544 test_samples: 8\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, train_size=0.8)\n",
    "\n",
    "print(\"train samples:\", X_train.shape[0], \"test_samples:\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 3776 samples, validate on 3776 samples\nEpoch 1/10\n - 1s - loss: 0.2439 - acc: 0.9555 - val_loss: 0.1756 - val_acc: 0.9688\nEpoch 2/10\n - 1s - loss: 0.2150 - acc: 0.9627 - val_loss: 0.1807 - val_acc: 0.9653\nEpoch 3/10\n - 1s - loss: 0.2086 - acc: 0.9635 - val_loss: 0.1877 - val_acc: 0.9624\nEpoch 4/10\n - 1s - loss: 0.1939 - acc: 0.9661 - val_loss: 0.1935 - val_acc: 0.9605\nEpoch 5/10\n - 1s - loss: 0.1888 - acc: 0.9672 - val_loss: 0.2018 - val_acc: 0.9603\nEpoch 6/10\n - 1s - loss: 0.1829 - acc: 0.9711 - val_loss: 0.2071 - val_acc: 0.9560\nEpoch 7/10\n - 1s - loss: 0.1751 - acc: 0.9717 - val_loss: 0.2213 - val_acc: 0.9507\nEpoch 8/10\n - 1s - loss: 0.1633 - acc: 0.9725 - val_loss: 0.2249 - val_acc: 0.9444\nEpoch 9/10\n - 1s - loss: 0.1618 - acc: 0.9751 - val_loss: 0.2243 - val_acc: 0.9478\nEpoch 10/10\n - 1s - loss: 0.1513 - acc: 0.9754 - val_loss: 0.2461 - val_acc: 0.9359\n"
    }
   ],
   "source": [
    "epochs = 10 #val_acc: 0.8021\n",
    "\n",
    "model = get_model()\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=epochs,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.fit_on_texts(train['text'])\n",
    "sequences = tokenizer.texts_to_sequences(test['text'])\n",
    "X_test = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n",
    "submission['target'] = np.round(preds).astype(int)\n",
    "\n",
    "submission.to_csv(os.path.join(data_dir, 'sub3.csv'), index=False)"
   ]
  }
 ]
}